<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Packages on Code R</title>
    <link>https://eliocamp.github.io/codigo-r/en/tags/packages/</link>
    <description>Recent content in Packages on Code R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es-es</language>
    <lastBuildDate>Sat, 06 Jul 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://eliocamp.github.io/codigo-r/en/tags/packages/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Why I love data.table</title>
      <link>https://eliocamp.github.io/codigo-r/en/2019/07/why-i-love-data-table/</link>
      <pubDate>Sat, 06 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://eliocamp.github.io/codigo-r/en/2019/07/why-i-love-data-table/</guid>
      <description>


&lt;p&gt;I‚Äôve been an R user for a few year now and the &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki&#34;&gt;data.table&lt;/a&gt; package has been my staple package for most of it. In this post I wanted to talk about why almost every script and RMarkdown report I write start with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;my-memory-issues&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;My memory issues&lt;/h1&gt;
&lt;p&gt;I started working on my &lt;a href=&#34;https://en.wikipedia.org/wiki/Licentiate_(degree)&#34;&gt;licenciate&lt;/a&gt; thesis (the argentinian equivalent to a Masters Degree) around mid 2016. I had been using R for school work and fun for some time and knew that I wanted to perform all my analysis in R and write my thesis in RMarkdown. In the end, &lt;a href=&#34;https://github.com/eliocamp/tesis/&#34;&gt;I did&lt;/a&gt; but in the process I had to learn new tools and also create my own (which materialised in the &lt;a href=&#34;https://eliocamp.github.io/metR/&#34;&gt;metR package&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The big problem I encountered early on was how to store and manipulate data. My main source of data were the output of atmospheric models which are stored usually in regularly spaced grids. The most natural way to store that kind of data would be in a multidimensional array like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file &amp;lt;- &amp;quot;~/DATOS/NCEP Reanalysis/air.mon.mean.nc&amp;quot;
subset &amp;lt;- list(level = 1000:800, 
               time = c(&amp;quot;1979-01-01&amp;quot;, &amp;quot;2018-12-01&amp;quot;))
temperature &amp;lt;- metR::ReadNetCDF(file, 
                                subset = subset,
                                out = &amp;quot;array&amp;quot;)[[1]]
str(temperature)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  num [1:144, 1:73, 1:3, 1:473] -30.5 -30.5 -30.5 -30.5 -30.5 ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 4
##   ..$ lon  : chr [1:144] &amp;quot;0&amp;quot; &amp;quot;2.5&amp;quot; &amp;quot;5&amp;quot; &amp;quot;7.5&amp;quot; ...
##   ..$ lat  : chr [1:73] &amp;quot;90&amp;quot; &amp;quot;87.5&amp;quot; &amp;quot;85&amp;quot; &amp;quot;82.5&amp;quot; ...
##   ..$ level: chr [1:3] &amp;quot;1000&amp;quot; &amp;quot;925&amp;quot; &amp;quot;850&amp;quot;
##   ..$ time : chr [1:473] &amp;quot;1979-01-01&amp;quot; &amp;quot;1979-02-01&amp;quot; &amp;quot;1979-03-01&amp;quot; &amp;quot;1979-04-01&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is very memory-efficient, but it doesn‚Äôt play well with a tidydata framework. Subsetting, filtering and operating on groups using arrays is rather awkward ‚Äìnot to mention that dimensions can only be characters! Furthermore, I had to transform it to a dataframe each time I wanted to plot it with ggplot2. What I needed was something more like this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temperature &amp;lt;- metR::ReadNetCDF(file, subset = subset)
str(temperature)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;data.table&amp;#39; and &amp;#39;data.frame&amp;#39;:   14916528 obs. of  5 variables:
##  $ level: num  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...
##  $ lat  : num  90 90 90 90 90 90 90 90 90 90 ...
##  $ lon  : num  0 2.5 5 7.5 10 12.5 15 17.5 20 22.5 ...
##  $ air  : num  -30.5 -30.5 -30.5 -30.5 -30.5 ...
##  $ time : POSIXct, format: &amp;quot;1979-01-01&amp;quot; &amp;quot;1979-01-01&amp;quot; ...
##  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem is that this representation is much less memory-efficient and my aging laptop couldn‚Äôt handle it. While it would eventually read it, even the simplest operation would crash my R session. This was due to the fact that R loooves to &lt;a href=&#34;http://adv-r.had.co.nz/memory.html#modification&#34;&gt;copy on modify&lt;/a&gt; and this is deadly if you‚Äôre dealing with data that fits on your memory but just barely.&lt;/p&gt;
&lt;p&gt;Enter data.table and its &lt;a href=&#34;https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reference-semantics.html&#34;&gt;modify by reference&lt;/a&gt; functionality. Unlike regular data.frames or tibbles, data.table objects can be easily modified without copying the entire object! And this means that you can safely work with objects that take more than half your available RAM.&lt;/p&gt;
&lt;p&gt;For this reason I often say that without data.table I wouldn‚Äôt have gotten my degree!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;come-for-the-performance-stay-for-the-syntax&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Come for the performance, stay for the syntax&lt;/h1&gt;
&lt;p&gt;But while my introduction to data.table was inspired by the need for memory optimisation, I quickly learned to love it‚Äôs minimalistic syntax.&lt;/p&gt;
&lt;p&gt;The basic form of data.table syntax is a very elegant extension of the classic data.frame. This is great because if you already use data.frames, then there‚Äôs no need to learn about a whole nother family of functions to do what you already did. In fact, data.tables are mostly just smarter data.frames. For example, if I wanted to filter only the northern hemisphere on my temperature dataset, with a regular data.frame I would have to use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temperature_df &amp;lt;- as.data.frame(temperature)
head(temperature_df[temperature_df$lat &amp;gt;= 0, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   level lat  lon       air       time
## 1  1000  90  0.0 -30.49999 1979-01-01
## 2  1000  90  2.5 -30.49999 1979-01-01
## 3  1000  90  5.0 -30.49999 1979-01-01
## 4  1000  90  7.5 -30.49999 1979-01-01
## 5  1000  90 10.0 -30.49999 1979-01-01
## 6  1000  90 12.5 -30.49999 1979-01-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But who‚Äôs got the time to write all that? I can barely stay awake after typing &lt;code&gt;temperature_df&lt;/code&gt; so many times üò¥! data.table is smart enough to realise that when I write ‚Äúlat‚Äù inside my data, I‚Äôm talking about the column whose name is ‚Äúlat‚Äù ‚Äìwhat else could I mean? It‚Äôs also smart enough that if I omit that last comma, it knows that I want every column (good riddance, ‚Äúundefined columns selected‚Äù!). So it all reduces to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(temperature[lat &amp;gt;= 0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    level lat  lon       air       time
## 1:  1000  90  0.0 -30.49999 1979-01-01
## 2:  1000  90  2.5 -30.49999 1979-01-01
## 3:  1000  90  5.0 -30.49999 1979-01-01
## 4:  1000  90  7.5 -30.49999 1979-01-01
## 5:  1000  90 10.0 -30.49999 1979-01-01
## 6:  1000  90 12.5 -30.49999 1979-01-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Isn‚Äôt that gorgeous? But there‚Äôs even more. The second argument inside the brackets allows one to select columns, so if I wanted to get the mean temperature, I could write this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(temperature_df[, &amp;quot;air&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.916081&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But with all those quotes I fear for the integrity of my ‚Äúshift‚Äù and ‚Äútwo‚Äù keys. Also, what if I wanted to apply complex operation on multiple columns? I would be repeating &lt;code&gt;temperature_df&lt;/code&gt; like a broken record while drowning in a sea of quotation marks! Again, since data.table is smart enough to know that when I‚Äôm inside a data.table I‚Äôm usually operating on its columns, I can just write this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temperature[, mean(air)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.916081&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Exquisite! The beautiful thing is that this works with &lt;strong&gt;any&lt;/strong&gt; function, which means that, again, I can apply all my previous base R knowledge without having to learn a whole new set of functions or operations.&lt;/p&gt;
&lt;p&gt;The last wonderful basic building block of data.table syntax is the &lt;code&gt;by&lt;/code&gt; argument. I often need to split the data in groups, apply some function and the join it all together. Using a normal data.frame this could be done artisanally with a for loop, or the more industrialised &lt;code&gt;by()&lt;/code&gt; function or &lt;code&gt;tapply()&lt;/code&gt; (maybe, I‚Äôve never really understood how it works). But not only would I hurt my hand due to repetitive typing, but I would also fall prey to memory issues. With data.table, applying any function to each group of the data is a breeze:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(temperature[, mean(air), by = .(lat, level)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     lat level        V1
## 1: 90.0  1000 -15.11903
## 2: 87.5  1000 -15.18808
## 3: 85.0  1000 -15.07319
## 4: 82.5  1000 -14.25968
## 5: 80.0  1000 -12.75084
## 6: 77.5  1000 -11.06509&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With just a slight change I can create a new column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temperature[, mean_air := mean(air), by = .(lat, level)]
head(temperature)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    level lat  lon       air       time  mean_air
## 1:  1000  90  0.0 -30.49999 1979-01-01 -15.11903
## 2:  1000  90  2.5 -30.49999 1979-01-01 -15.11903
## 3:  1000  90  5.0 -30.49999 1979-01-01 -15.11903
## 4:  1000  90  7.5 -30.49999 1979-01-01 -15.11903
## 5:  1000  90 10.0 -30.49999 1979-01-01 -15.11903
## 6:  1000  90 12.5 -30.49999 1979-01-01 -15.11903&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here lays maybe the biggest departure from the classic data.frame. The &lt;code&gt;:=&lt;/code&gt; operator adds columns by reference, which means that there‚Äôs no need to assign the result to a new variable! That is, there‚Äôs no need to use &lt;code&gt;temperature &amp;lt;- temperature[, mean_air := mean(air), by = .(lat, level)]&lt;/code&gt;. If you remember all the stuff above about memory efficiency then you understand why it‚Äôs a very useful feature for me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-is-not-a-pipe&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;This is not a pipe&lt;/h1&gt;
&lt;p&gt;data.table has its own idiomatic way of chaining operation but I prefer to use pipes (&lt;code&gt;%&amp;gt;%&lt;/code&gt;). The trick is to realised that when using a pipe, the dot (&lt;code&gt;.&lt;/code&gt;) is a stand-in for the previous result. In practice this means that data.table operations can be chained thus:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
library(ggplot2)
temperature %&amp;gt;% 
   .[level == 1000] %&amp;gt;% 
   .[, mean(air), by = .(lat, lon)] %&amp;gt;% 
   .[lat &amp;gt; 0] %&amp;gt;% 
   ggplot(aes(lon, lat)) +
   geom_raster(aes(fill = V1), interpolate = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../../post/2019-07-06-why-i-love-data-table.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;and-more&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;‚Ä¶and more!&lt;/h1&gt;
&lt;p&gt;Of course this only scratches the surface of all the goodness of the data.table package. Inside the hood there are lots of optimisations to give it extra speed. It‚Äôs got special symbols that allow for more complex operations and optimised logical operators such as &lt;code&gt;%like%&lt;/code&gt; and &lt;code&gt;%between%&lt;/code&gt;. The &lt;code&gt;fread()&lt;/code&gt; and &lt;code&gt;fwrite()&lt;/code&gt; functions not only are insanely fast but also are packed with functionality. And so on‚Ä¶&lt;/p&gt;
&lt;p&gt;So why I love data.table? I love that allows me to work with big and small datasets with the same elegant syntax and with great performance without even thinking about it. It is a wonderful package and you should give it a try!&lt;/p&gt;
&lt;div id=&#34;now-i-love-data.table-too&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;‚ÄúNow I love data.table too!‚Äù&lt;/h3&gt;
&lt;p&gt;If my love for data.table rubbed on you even a little bit, then a good summary of the basic functionality is the &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Getting-started&#34;&gt;Getting Started&lt;/a&gt; set of articles. If you already know a the basics and want to take your skills to the next level, the &lt;a href=&#34;http://brooksandrew.github.io/simpleblog/articles/advanced-data-table/&#34;&gt;Advanced tips and tricks with data.table&lt;/a&gt;, is chock full of advanced tricks.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Statistical metamerism</title>
      <link>https://eliocamp.github.io/codigo-r/en/2019/01/statistical-metamerism/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://eliocamp.github.io/codigo-r/en/2019/01/statistical-metamerism/</guid>
      <description>


&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/eliocamp/metamer&#34;&gt;metamer&lt;/a&gt; package implements &lt;span class=&#34;citation&#34;&gt;Matejka and Fitzmaurice (&lt;a href=&#34;#ref-Matejka2017&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; algorithm for generating datasets with distinct appearance but identical statistical properties. I propose to call them ‚Äúmetamers‚Äù as an analogy with the colorimetry concept.&lt;/p&gt;
&lt;div id=&#34;metamers-in-vision&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metamers in vision&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; a prism separating white light into its component wavelengths. It is an &lt;em&gt;image&lt;/em&gt; of a prism separating white light into its component wavelengths.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:prism&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../images/Prism_flat_rainbow.jpg&#34; alt=&#34;C&#39;est ne pas un prisme.&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Fig. 1: C‚Äôest ne pas un prisme.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is not just a Magritte-style observation. The important distinction comes into play when you realise that the monitor you are looking at has just three LEDs that emit light in just three wavelengths (sort of). How can it still reproduce a full spectrum of light? It doesn‚Äôt. For each (approximately) monochromatic colour in that rainbow, your monitor is actually emitting an unique mixture of red, green and blue light that tricks your visual system (and mine) into seeing the colour associated with that wavelength.&lt;/p&gt;
&lt;p&gt;How that works is unreasonably complex and beyond what I can explain in this article (I do recommend this &lt;a href=&#34;http://jamie-wong.com/post/color/&#34;&gt;amazing article&lt;/a&gt;, though) but the core insight is that our eyes have only three colour receptors that are sensible to wide range of short (S), medium (M) and long (L) wavelengths. Any spectrum distribution that reaches our eyes is reduced to just three numbers: the excitation of the S, M and L receptors. Hence, any spectrum distribution that excites them in the same way will be perceived as the same colour, even if they are wildly different. In colorimetry this is known as &lt;em&gt;metamerism&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Hunt &lt;a href=&#34;#ref-Hunt2004-7&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt;. The monochromatic yellow emitted by the prism looks to you identical as the red, green and blue mixture emitted by of your monitor even though their spectrum distribution is not even remotely similar. They are metamers.&lt;/p&gt;
&lt;p&gt;Coming up with metameric matches is the basis for colour reproduction in computer screens, printing and painting, but it also has a dark side. Two pigments can be metameric matches under certain light conditions but have very different colours when illuminated with another type light. This can be a problem, for example, when buying clothes in a store with artificial lighting and then wearing them outside.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;metamers-in-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metamers in statistics&lt;/h2&gt;
&lt;p&gt;Now let‚Äôs focus our attention on the famous Anscombe quartet&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:anscombe-plot&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../post/2018-12-18-metamerismo-estad%C3%ADstico.en_files/figure-html/anscombe-plot-1.png&#34; alt=&#34;Anscombe quartet&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Fig. 2: Anscombe quartet
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Even though they are very different datasets, the members of the quartet have the same mean and standard deviation of each variable as well as the correlation between the two &lt;span class=&#34;citation&#34;&gt;(Anscombe &lt;a href=&#34;#ref-Anscombe1973&#34;&gt;1973&lt;/a&gt;)&lt;/span&gt;. From the point of view of that statistical transformation, the four datasets look the same even though they are not even remotely similar. They are metamers.&lt;/p&gt;
&lt;p&gt;And exactly the same as metameric colour matches, statistical metamers reveal their differences when viewed under a new light. In this case, when plotted.&lt;/p&gt;
&lt;p&gt;The concept of ‚Äúdata with different graphs but same statistics‚Äù is still relevant, with multiple published papers describing methods for their creation &lt;span class=&#34;citation&#34;&gt;(e.g. Chatterjee and Firat &lt;a href=&#34;#ref-Chatterjee2007&#34;&gt;2007&lt;/a&gt;; Govindaraju and Haslett &lt;a href=&#34;#ref-Govindaraju2008&#34;&gt;2008&lt;/a&gt;; Haslett and Govindaraju &lt;a href=&#34;#ref-Haslett2009&#34;&gt;2009&lt;/a&gt;; Matejka and Fitzmaurice &lt;a href=&#34;#ref-Matejka2017&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;. In this post I will use the term ‚Äúmetamers‚Äù to refer to sets of datasets that have the same behaviour under some statistical transformation as an analogy with the colorimetry concept.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/eliocamp/metamer&#34;&gt;metamer&lt;/a&gt; package implements &lt;span class=&#34;citation&#34;&gt;Matejka and Fitzmaurice (&lt;a href=&#34;#ref-Matejka2017&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; algorithm to construct metamers. The main function, &lt;code&gt;metamerize()&lt;/code&gt;, generates metamers from an initial dataset and the statistical transformation that needs to be preserved. Optionally, it can take a function that will be minimised in each successive metamer.&lt;/p&gt;
&lt;p&gt;First, the function &lt;code&gt;delayed_with()&lt;/code&gt; is useful for defining the statistical transformation that need to be preserved. The four datasets in the Anscombe quartet share these properties up to three significant figures.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(metamer)

summ_fun &amp;lt;- delayed_with(mean_x = mean(x), 
                         mean_y = mean(y), 
                         sd_x = sd(x), 
                         sd_y = sd(y), 
                         cor_xy = cor(x, y))
summ_names &amp;lt;-  c(&amp;quot;$\\overline{x}$&amp;quot;, &amp;quot;$\\overline{y}$&amp;quot;, 
                 &amp;quot;$S_x$&amp;quot;,  &amp;quot;$S_y$&amp;quot;, &amp;quot;$r(x, y)$&amp;quot;)

anscombe[, as.list(signif(summ_fun(.SD), 3)), by = quartet] %&amp;gt;% 
   knitr::kable(col.names = c(&amp;quot;Quartet&amp;quot;, summ_names),
                escape = FALSE, 
                caption = &amp;quot;Statistical properties of the Anscombe quartet.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:anscombe-summ&#34;&gt;Tab. 1: &lt;/span&gt;Statistical properties of the Anscombe quartet.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Quartet&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\overline{x}\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\overline{y}\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_x\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_y\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(r(x, y)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.817&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To find metamers ‚Äúbetween‚Äù the first and second quartet, one can start from one and generate metamers that minimise the mean distance to the other. The &lt;code&gt;mean_dist_to()&lt;/code&gt; function is a handy utility for that case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extracts the first quartet and removes the `quartet` column.
start_data &amp;lt;- subset(anscombe, quartet == 1)
start_data$quartet &amp;lt;- NULL

# Extracts the second quartet and removes the `quartet` column.
target &amp;lt;- subset(anscombe, quartet == 2)
target$quartet &amp;lt;- NULL

set.seed(42)  # for reproducibility
metamers &amp;lt;- metamerize(start_data, 
                       preserve = summ_fun,
                       minimize = mean_dist_to(target), 
                       signif = 3,
                       change = &amp;quot;y&amp;quot;,
                       perturbation = 0.008, 
                       N = 30000)
print(metamers)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4690 metamers&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The process generates 4689 metamers plus the original dataset. Selecting only 10 of them with &lt;code&gt;trim()&lt;/code&gt; and applying &lt;code&gt;summ_fun()&lt;/code&gt; to each one, it is confirmed that they have the same properties up to three significant figures.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metamers %&amp;gt;% 
   trim(10) %&amp;gt;% 
   lapply(summ_fun) %&amp;gt;% 
   lapply(signif, digits = 3) %&amp;gt;% 
   do.call(rbind, .) %&amp;gt;% 
   knitr::kable(col.names = c(summ_names),
                caption = &amp;quot;Statistical properties of the generated metamers (rounded to three significant figures).&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:anscombe-metamers&#34;&gt;Tab. 2: &lt;/span&gt;Statistical properties of the generated metamers (rounded to three significant figures).&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\overline{x}\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\overline{y}\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_x\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(S_y\)&lt;/span&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(r(x, y)\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With &lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;gganimate&lt;/a&gt; it is possible to visualise the transformation. Every intermediate step is a metamer of the original.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gganimate)

metamers %&amp;gt;% 
   trim(100) %&amp;gt;% 
   as.data.frame() %&amp;gt;% 
   ggplot(aes(x, y)) +
   geom_point() +
   transition_manual(.metamer)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:anscombe-animate&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../post/2018-12-18-metamerismo-estad%C3%ADstico.en_files/figure-html/anscombe-animate-1.gif&#34; alt=&#34;Metamorphosys of the first two quartets.&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Fig. 3: Metamorphosys of the first two quartets.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The discussion around statistical metamerism is usually framed as the importance of visualising data instead of relying on summary statistics. Anscombe created his quartet to rebut the idea that ‚Äúnumerical calculations are exact, but graphs are rough‚Äù. Now this is still the interpretation of the phenomenon:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:tweet&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../images/datasaurus_tweet.png&#34; alt=&#34;Download the datasaurus. ([Tweet](https://twitter.com/albertocairo/status/770267777169035264))&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Fig. 4: Download the datasaurus. (&lt;a href=&#34;https://twitter.com/albertocairo/status/770267777169035264&#34;&gt;Tweet&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;However, I believe there is a more fundamental principle at play. The problem with &lt;em&gt;summary statistics&lt;/em&gt; is the &lt;em&gt;summary&lt;/em&gt; part. In many cases, the role of statistics is to sum up data. To take a big set of observations that cannot be grasped in their entirety because the limitations of our comprehension, and condense them into a few numbers or properties that we can easily get. The problem is that what is gained in understanding is lost in information.&lt;/p&gt;
&lt;p&gt;For example, a complete earnings census is a huge amount of data, but as raw numbers they are impossible to understand. One can start by taking the average (first moment) to get some idea of the ‚Äútypical‚Äù earning of a citizen. Of course, this single number hides a great deal of income inequality, so one can compute the standard deviation (second moment) to get an idea of the variability. It is very likely, though, that the distribution is not symmetrical, and one can use the skewness (third moment) to quantify that.&lt;/p&gt;
&lt;p&gt;With each subsequent moment one can get a richer picture of the underlying data. The limit is when one has the same amount of moments as the sample size. A single univariate sample of size N can be unequivocally described by its N first moments. This makes sense intuitively ‚Äìwhy should you need more than N numbers to describe N numbers?‚Äì but it can be &lt;a href=&#34;https://math.stackexchange.com/questions/3033407/is-a-sample-of-size-n-uniquely-described-by-n-sample-moments&#34;&gt;demonstrated&lt;/a&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In other words, the transformation ‚Äúfirst N moments‚Äù has no metamers for samples smaller than N, except for any permutation of the same sample (but see &lt;a href=&#34;#fn1&#34;&gt;1&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But this property is not exclusive to statistical moments. The same goes for the fourier transform, principal component analysis, factor analysis, clustering, etc‚Ä¶.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; The issue is not plots vs.¬†numbers but ‚Äúall the numbers‚Äù vs. ‚Äújust some numbers‚Äù. The big advantage of plots is that they can show an enormous amount of numbers efficiently and intuitively, in addition allowing to see a &lt;em&gt;gestalt&lt;/em&gt; that is impossible to get by just looking at series of numbers.&lt;/p&gt;
&lt;p&gt;With this in mind, it is possible to predict when it will be easy to find metamers and in which cases it is a mathematical impossibility. For example, it is impossible to find metamers of a sample of size 10 that preserves 10 moments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42) 
start_data &amp;lt;- data.frame(x = rnorm(10))

metamerize(start_data, 
           moments_n(1:10),
           signif = 3,
           perturbation = 0.05,
           N = 30000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 1 metamers&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But it is possible to find metamers that preserve just two.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42) 
metamerize(start_data, 
           moments_n(1:2), 
           signif = 3,
           perturbation = 0.01,
           N = 30000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 310 metamers&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Boxplots try to represent a sample with about 5 numbers. Hence, it is expected to have metamerism for samples with &lt;span class=&#34;math inline&#34;&gt;\(N&amp;gt;5\)&lt;/span&gt;. A density estimation using parametric methods, on the other hand, can be evaluated at potentially infinite points even for small samples. The possibility of metamerism in this case depends on the ‚Äúresolution‚Äù with which the curve is described. If it is rendered with fewer points than the sample size, then it will metamerise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coarse_density &amp;lt;- function(data) {
   density(data$x, n = 16)$y
}
set.seed(42) 
metamerize(data.frame(x = rnorm(100)),
           preserve = coarse_density,
           N = 5000,
           signif = 3,
           perturbation = 0.001)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 11 metamers&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, if it is rendered with more points, it will not metamerise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;highdef_density &amp;lt;- function(data) {
   density(data$x, n = 200)$y
}
set.seed(42) 
metamerize(data.frame(x = rnorm(100)),
           preserve = highdef_density,
           N = 5000,
           signif = 3,
           perturbation = 0.001)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 1 metamers&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The general principle works, but it is not complete. Imagine a statistical transformation defined as the sample mean repeated N times. Even though it returns N numbers from a N-sized sample, it does not have more information than just the mean. Generating metamers is then trivial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_n_times &amp;lt;- function(data) {
   rep(mean(data$x), length.out = length(data$x))
}
set.seed(42) 
metamerize(data.frame(x = rnorm(100)),
           preserve = mean_n_times,
           perturbation = 0.1, 
           N = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 43 metamers&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This motivates to define the category of ‚Äúeffective‚Äù statistical transformations as transformation that can uniquely describe a univariate sample of size N with, at most, N numbers. Under this definition, ‚Äúthe first N moments‚Äù is effective, while ‚Äúthe first moment repeated N times‚Äù is no. At this point, this is pure speculation, so take it with a grain of salt.&lt;/p&gt;
&lt;p&gt;It is worth noticing that when searching for metamers empirically there is a need to set the numerical tolerance (with the argument &lt;code&gt;signif&lt;/code&gt;). Being pedantic, these are more like ‚Äúsemi-metamers‚Äù than true metamers. With a high enough tolerance it is possible to find (semi) metamers even when it should not be possible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42) 
metamerize(data.frame(x = rnorm(3)),
                       moments_n(1:4), 
                       signif = 1, 
                       perturbation = 0.001, 
                       N = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 1000 metamers&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;advanced-metamers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Advanced metamers&lt;/h2&gt;
&lt;p&gt;I would like to close with a showcase of some utilities in the metamer package. &lt;code&gt;draw_data()&lt;/code&gt; opens up a shiny interface to freehand draw datasets with an optional dataset as backdrop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;start_data &amp;lt;- subset(datasauRus::datasaurus_dozen, dataset == &amp;quot;dino&amp;quot;)
start_data$dataset &amp;lt;- NULL

smiley &amp;lt;- draw_data(start_data)
simley$.group &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:draw-data&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../images/draw_data.png&#34; alt=&#34;`draw_data()` interface.&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Fig. 5: &lt;code&gt;draw_data()&lt;/code&gt; interface.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Moreover, &lt;code&gt;metamerize()&lt;/code&gt; can be piped, saving the parameters of each call (except &lt;code&gt;N&lt;/code&gt; and &lt;code&gt;trim&lt;/code&gt;). This way one can perform sequences.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- subset(datasauRus::datasaurus_dozen, dataset == &amp;quot;x_shape&amp;quot;)
X$dataset &amp;lt;- NULL

star &amp;lt;- subset(datasauRus::datasaurus_dozen, dataset == &amp;quot;star&amp;quot;)
star$dataset &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metamers &amp;lt;- metamerize(start_data, 
                       preserve = delayed_with(mean(x), mean(y), cor(x, y)),
                       minimize = mean_dist_to(smiley), 
                       perturbation = 0.08,
                       N = 30000,
                       trim = 150) %&amp;gt;% 
   metamerize(minimize = NULL, 
              N = 3000, trim = 10) %&amp;gt;% 
   metamerize(minimize = mean_dist_to(X), 
              N = 30000, trim = 150) %&amp;gt;% 
   metamerize(minimize = NULL, 
              N = 3000, trim = 10) %&amp;gt;% 
   metamerize(minimize = mean_dist_to(star), 
              N = 30000, trim = 150) %&amp;gt;%
   metamerize(minimize = NULL, 
              N = 3000, trim = 10) %&amp;gt;% 
   metamerize(minimize = mean_dist_to(start_data),
              N = 30000, trim = 150)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This metamers show the datasaurus metamorphosing into different figures, always preserving the same statistical properties. This replicates &lt;a href=&#34;https://www.autodeskresearch.com/publications/samestats&#34;&gt;Justin Matejka‚Äôs y George Fitzmaurice‚Äôs animation&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metamers %&amp;gt;% 
   as.data.frame() %&amp;gt;% 
   ggplot(aes(x, y)) +
   geom_point() +
   transition_manual(.metamer)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:metamer-chain-anim&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../../post/2018-12-18-metamerismo-estad%C3%ADstico.en_files/figure-html/metamer-chain-anim-1.gif&#34; alt=&#34;Datasaurus metamorphisis.&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Fig. 6: Datasaurus metamorphisis.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Anscombe1973&#34;&gt;
&lt;p&gt;Anscombe, F J. 1973. ‚ÄúGraphs in Statistical Analysis.‚Äù &lt;em&gt;The American Statistician&lt;/em&gt; 27 (1): 17‚Äì21. &lt;a href=&#34;https://doi.org/10.1007/978-3-540-71915-1_35&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/978-3-540-71915-1_35&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Chatterjee2007&#34;&gt;
&lt;p&gt;Chatterjee, Sangit, and Aykut Firat. 2007. ‚ÄúGenerating data with identical statistics but dissimilar graphics: A follow up to the anscombe dataset.‚Äù &lt;em&gt;American Statistician&lt;/em&gt; 61 (3): 248‚Äì54. &lt;a href=&#34;https://doi.org/10.1198/000313007X220057&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1198/000313007X220057&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Govindaraju2008&#34;&gt;
&lt;p&gt;Govindaraju, K., and S. J. Haslett. 2008. ‚ÄúIllustration of regression towards the means.‚Äù &lt;em&gt;International Journal of Mathematical Education in Science and Technology&lt;/em&gt; 39 (4): 544‚Äì50. &lt;a href=&#34;https://doi.org/10.1080/00207390701753788&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/00207390701753788&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Haslett2009&#34;&gt;
&lt;p&gt;Haslett, S. J., and K. Govindaraju. 2009. ‚ÄúCloning data: Generating datasets with exactly the same multiple linear regression fit.‚Äù &lt;em&gt;Australian and New Zealand Journal of Statistics&lt;/em&gt; 51 (4): 499‚Äì503. &lt;a href=&#34;https://doi.org/10.1111/j.1467-842X.2009.00560.x&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/j.1467-842X.2009.00560.x&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Hunt2004-7&#34;&gt;
&lt;p&gt;Hunt, R. W. G. 2004. ‚ÄúThe Colour Triangle.‚Äù In &lt;em&gt;The Reproduction of Colour&lt;/em&gt;, 6th ed., 68‚Äì91. &lt;a href=&#34;https://doi.org/10.1002/0470024275&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1002/0470024275&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Matejka2017&#34;&gt;
&lt;p&gt;Matejka, Justin, and George Fitzmaurice. 2017. ‚ÄúSame Stats, Different Graphs.‚Äù &lt;em&gt;Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI ‚Äô17&lt;/em&gt;, 1290‚Äì4. &lt;a href=&#34;https://doi.org/10.1145/3025453.3025912&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1145/3025453.3025912&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Technically, the solution is unique up to any permutation. This is not an accident. If the order matters, then it is a case of bivariate samples (each ‚Äúdatum‚Äù is actually a pair of values (x; y)). Intuition tells that besides the moment of each variable, the joint moments (covariance and such) are needed. So it seems plausible that in this case the matrix &lt;span class=&#34;math inline&#34;&gt;\(A^{N\times N}\)&lt;/span&gt;, where the element in the ith row and jth column is &lt;span class=&#34;math inline&#34;&gt;\(x^iy^j\)&lt;/span&gt; would be needed; which implies the need of &lt;span class=&#34;math inline&#34;&gt;\(N^2 -1\)&lt;/span&gt; moments.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The fourier transform case is interesting because it describes an &lt;em&gt;ordered&lt;/em&gt; sample of size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; with two ordered series of &lt;span class=&#34;math inline&#34;&gt;\(N/2\)&lt;/span&gt; numbers (one real and one imaginary) which sum up to &lt;span class=&#34;math inline&#34;&gt;\(2N\)&lt;/span&gt; numbers (the two series plus their respective order). This is much less than the assumed &lt;span class=&#34;math inline&#34;&gt;\(N^1-1\)&lt;/span&gt; needed in general. I suspect that this is because for this to happen, a regularly sampled series is needed. With this restriction, the fourier transform can ‚Äúcompress‚Äù the information.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>